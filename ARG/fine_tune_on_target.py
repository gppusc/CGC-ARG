import os
import sys

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Subset, WeightedRandomSampler
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import train_test_split
from transformers import AutoConfig
import numpy as np
from tqdm import tqdm
from AutoCNN_NewMoE_ASL import GCM_MultiLabelModel
from asym import AsymmetricLoss
from collections import Counter

sys.path.append('data')
from Dataset import ProteinDataset

# -------------------------------
# ğŸ”§ é…ç½®
# -------------------------------
model_dir = "outputs/AutoCNN_NewMoE_ASL_outputs_6/best_model"
target_data_path = "processed_data/my_test_encoded_dataset.pt"
batch_size = 4
num_epochs = 50
patience = 5
lr = 1e-5
save_dir = "finetune_on_target_outputs"
os.makedirs(save_dir, exist_ok=True)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# -------------------------------
# âœ… åŠ è½½æ¨¡å‹
# -------------------------------
def load_model(model_class, model_dir, device):
    config = AutoConfig.from_pretrained(model_dir)
    model = model_class(config=config)
    state_dict = torch.load(os.path.join(model_dir, "pytorch_model.bin"), map_location=device)
    new_state_dict = {}
    for key, value in state_dict.items():
        new_key = key.replace("module.", "") if key.startswith("module.") else key
        new_state_dict[new_key] = value
    model.load_state_dict(new_state_dict)
    return model.to(device)


print("************åŠ è½½åŸå§‹æ¨¡å‹**************")
model = load_model(GCM_MultiLabelModel, model_dir, device)

# -------------------------------
# âœ… æ•°æ®å‡†å¤‡ (ä¸“æ³¨äºäºŒåˆ†ç±»ä»»åŠ¡)
# -------------------------------
print("************åŠ è½½å¤–éƒ¨æ•°æ®é›†**************")
target_dataset = torch.load(target_data_path)


# ç»Ÿè®¡äºŒåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ
def get_binary_class_distribution(dataset):
    """è·å–äºŒåˆ†ç±»æ ‡ç­¾çš„åˆ†å¸ƒ"""
    class_counts = Counter()
    for item in dataset:
        label = item['is_arg']
        class_counts[label] += 1

    total_samples = len(dataset)
    print("\nğŸ”¬ äºŒåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    print(f"éè€è¯åŸºå›  (0): {class_counts[0]} ä¸ªæ ·æœ¬ ({class_counts[0] / total_samples * 100:.2f}%)")
    print(f"è€è¯åŸºå›  (1): {class_counts[1]} ä¸ªæ ·æœ¬ ({class_counts[1] / total_samples * 100:.2f}%)")
    print(f"æ€»æ ·æœ¬æ•°: {total_samples}")

    return class_counts


class_counts = get_binary_class_distribution(target_dataset)

# åˆ›å»ºæ ·æœ¬æƒé‡ä»¥å¹³è¡¡ç±»åˆ«
sample_weights = []
for item in target_dataset:
    # å¯¹äºå°‘æ•°ç±»åˆ«æ ·æœ¬ï¼Œå¢åŠ æƒé‡
    weight = 1.0
    if item['is_arg'] == 1:  # è€è¯åŸºå› æ ·æœ¬
        weight = max(weight, class_counts[0] / class_counts[1])  # åæ¯”äºç±»åˆ«é¢‘ç‡

    sample_weights.append(weight)

# ä½¿ç”¨åˆ†å±‚æŠ½æ ·ç¡®ä¿ç±»åˆ«åˆ†å¸ƒå‡è¡¡
binary_labels = [item['is_arg'] for item in target_dataset]
indices = list(range(len(target_dataset)))
train_indices, val_indices = train_test_split(
    indices,
    test_size=0.1,
    stratify=binary_labels,  # ä½¿ç”¨äºŒåˆ†ç±»æ ‡ç­¾è¿›è¡Œåˆ†å±‚
    random_state=42
)

# åˆ›å»ºå­é›†
target_train = Subset(target_dataset, train_indices)
target_val = Subset(target_dataset, val_indices)

# éªŒè¯åˆ†å±‚æŠ½æ ·æ•ˆæœ
print("\nğŸ”¬ åˆ†å±‚æŠ½æ ·åäºŒåˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ:")
print("è®­ç»ƒé›†åˆ†å¸ƒ:")
get_binary_class_distribution(target_train)
print("\néªŒè¯é›†åˆ†å¸ƒ:")
get_binary_class_distribution(target_val)

# åˆ›å»ºå¸¦æƒé‡çš„éšæœºé‡‡æ ·å™¨ä»¥å¹³è¡¡è®­ç»ƒé›†
train_weights = [sample_weights[i] for i in train_indices]
train_sampler = WeightedRandomSampler(
    weights=train_weights,
    num_samples=len(train_weights),
    replacement=True
)

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
train_loader = DataLoader(
    target_train,
    batch_size=batch_size,
    sampler=train_sampler,  # ä½¿ç”¨åŠ æƒé‡‡æ ·å™¨
    pin_memory=True
)
val_loader = DataLoader(
    target_val,
    batch_size=batch_size,
    pin_memory=True
)


# -------------------------------
# ğŸ§ª äºŒåˆ†ç±»è¯„ä¼°å‡½æ•°
# -------------------------------
def evaluate_binary(model, dataloader):
    model.eval()
    all_preds = []
    all_probs = []
    all_true = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="ğŸ” Evaluating", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            resistance_logits, _, _, _ = outputs.logits  # åªå…³æ³¨äºŒåˆ†ç±»è¾“å‡º

            # è·å–é¢„æµ‹æ¦‚ç‡å’Œç±»åˆ«
            probs = resistance_logits.sigmoid().cpu().numpy()
            preds = (probs > 0.5).astype(int)

            true_labels = batch['is_arg'].cpu().numpy()

            all_preds.append(preds)
            all_probs.append(probs)
            all_true.append(true_labels)

    preds = np.concatenate(all_preds)
    probs = np.concatenate(all_probs)
    true = np.concatenate(all_true)

    # è®¡ç®—åˆ†ç±»æŠ¥å‘Š
    report = classification_report(
        true, preds,
        target_names=["éè€è¯åŸºå› ", "è€è¯åŸºå› "],
        zero_division=0
    )

    # è®¡ç®—AUROC
    if len(np.unique(true)) >= 2:  # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
        auroc = roc_auc_score(true, probs)
    else:
        auroc = 0.0
        print("âš ï¸ æ— æ³•è®¡ç®—AUROC - éªŒè¯é›†ä¸­ç¼ºå°‘æ­£æ ·æœ¬æˆ–è´Ÿæ ·æœ¬")

    return report, auroc


# -------------------------------
# ğŸ” äºŒåˆ†ç±»å¾®è°ƒè®­ç»ƒ
# -------------------------------
optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=2,
    verbose=True
)


# äºŒåˆ†ç±»æŸå¤±å‡½æ•°
def binary_loss(outputs, resistance_labels):
    resistance_logits, _, _, _ = outputs.logits
    return nn.BCEWithLogitsLoss()(resistance_logits, resistance_labels.float())


best_auroc = 0
no_improve = 0

for epoch in range(1, num_epochs + 1):
    model.train()
    total_loss = 0
    print(f"\nğŸ” Epoch {epoch}/{num_epochs}")
    for batch in tqdm(train_loader, desc="ğŸ§ª Training", leave=False):
        optimizer.zero_grad()

        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        resistance_labels = batch['is_arg'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        loss = binary_loss(outputs, resistance_labels)

        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    report, auroc = evaluate_binary(model, val_loader)
    print(f"\nğŸ“‰ Epoch {epoch} | Loss: {total_loss:.4f} | Val AUROC: {auroc:.4f}")
    print("ğŸ§¬ äºŒåˆ†ç±»æŠ¥å‘Š:")
    print(report)

    # æ›´æ–°å­¦ä¹ ç‡
    scheduler.step(auroc)
    current_lr = optimizer.param_groups[0]['lr']
    print(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")

    if auroc > best_auroc:
        best_auroc = auroc
        no_improve = 0
        model_save_path = os.path.join(save_dir, "best_model")
        os.makedirs(model_save_path, exist_ok=True)
        torch.save(model.state_dict(), os.path.join(model_save_path, "pytorch_model.bin"))
        print("âœ… Best model_1 saved.")
    else:
        no_improve += 1
        if no_improve >= patience:
            print("â¹ï¸ Early stopping triggered.")
            break

# -------------------------------
# ğŸ§ª Final æµ‹è¯•
# -------------------------------
print("\nğŸ” åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
best_model_path = os.path.join(save_dir, "best_model")
model = load_model(GCM_MultiLabelModel, best_model_path, device)
report, final_auroc = evaluate_binary(model, val_loader)

print("\nğŸ“Š ç›®æ ‡éªŒè¯é›†ä¸Šçš„æœ€ç»ˆè¯„ä¼°")
print("ğŸ§¬ äºŒåˆ†ç±»æŠ¥å‘Š:")
print(report)
print(f"ğŸ¯ æœ€ç»ˆAUROC: {final_auroc:.4f}")